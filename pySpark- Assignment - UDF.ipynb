{"cells":[{"cell_type":"code","source":["print(\"UDF Assignment\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e97cb8f6-9238-404d-a07f-9dd1cd6251b3"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"UDF Assignment\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["UDF Assignment\n"]}}],"execution_count":0},{"cell_type":"code","source":["# Databricks notebook source\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import col, lit, udf\nfrom pyspark.sql.types import IntegerType,DoubleType\nspark = SparkSession.builder.appName(\"Spark DataFrames\").getOrCreate()\n\n# COMMAND ----------\ndf = spark.read.options(header='True', inferSchema='True').csv('/FileStore/tables/OfficeData.csv')\ndf.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c36ac404-c7e4-4964-9a5e-868d8a836cee"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"+-------------+----------+-----+------+---+-----+\n|employee_name|department|state|salary|age|bonus|\n+-------------+----------+-----+------+---+-----+\n|        James|     Sales|   NY| 90000| 34|10000|\n|      Michael|     Sales|   NY| 86000| 56|20000|\n|       Robert|     Sales|   CA| 81000| 30|23000|\n|        Maria|   Finance|   CA| 90000| 24|23000|\n|        Raman|   Finance|   CA| 99000| 40|24000|\n|        Scott|   Finance|   NY| 83000| 36|19000|\n|          Jen|   Finance|   NY| 79000| 53|15000|\n|         Jeff| Marketing|   CA| 80000| 25|18000|\n|        Kumar| Marketing|   NY| 91000| 50|21000|\n+-------------+----------+-----+------+---+-----+\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+-------------+----------+-----+------+---+-----+\n|employee_name|department|state|salary|age|bonus|\n+-------------+----------+-----+------+---+-----+\n|        James|     Sales|   NY| 90000| 34|10000|\n|      Michael|     Sales|   NY| 86000| 56|20000|\n|       Robert|     Sales|   CA| 81000| 30|23000|\n|        Maria|   Finance|   CA| 90000| 24|23000|\n|        Raman|   Finance|   CA| 99000| 40|24000|\n|        Scott|   Finance|   NY| 83000| 36|19000|\n|          Jen|   Finance|   NY| 79000| 53|15000|\n|         Jeff| Marketing|   CA| 80000| 25|18000|\n|        Kumar| Marketing|   NY| 91000| 50|21000|\n+-------------+----------+-----+------+---+-----+\n\n"]}}],"execution_count":0},{"cell_type":"code","source":["def Increment(state,salary,bonus):\n  if state =='NY':\n    return (salary*0.1 + bonus*0.05)\n  elif state == 'CA':\n    return (salary*0.12 + bonus*0.03)\n  else:\n    return 0"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b809bf50-0562-4954-95ab-5ff68355b988"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["Increment = udf(lambda x,y,z : Increment(x,y,z), DoubleType())\ndf.withColumn(\"Increment\", Increment(df.state,df.salary,df.bonus)).show()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"0cc7977d-697b-440d-83bc-a16e9c6412f3"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mPythonException\u001B[0m                           Traceback (most recent call last)\n\u001B[0;32m<command-1699698136113742>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[0mIncrement\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mudf\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;32mlambda\u001B[0m \u001B[0mx\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0my\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0mz\u001B[0m \u001B[0;34m:\u001B[0m \u001B[0mIncrement\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mx\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0my\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0mz\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mDoubleType\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 2\u001B[0;31m \u001B[0mdf\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mwithColumn\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"Increment\"\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mIncrement\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdf\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mstate\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0mdf\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msalary\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0mdf\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mbonus\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mshow\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\n\u001B[0;32m/databricks/spark/python/pyspark/sql/dataframe.py\u001B[0m in \u001B[0;36mshow\u001B[0;34m(self, n, truncate, vertical)\u001B[0m\n\u001B[1;32m    500\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    501\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0misinstance\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtruncate\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mbool\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32mand\u001B[0m \u001B[0mtruncate\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 502\u001B[0;31m             \u001B[0mprint\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_jdf\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mshowString\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mn\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;36m20\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mvertical\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    503\u001B[0m         \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    504\u001B[0m             \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.1-src.zip/py4j/java_gateway.py\u001B[0m in \u001B[0;36m__call__\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m   1302\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1303\u001B[0m         \u001B[0manswer\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mgateway_client\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msend_command\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcommand\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1304\u001B[0;31m         return_value = get_return_value(\n\u001B[0m\u001B[1;32m   1305\u001B[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001B[1;32m   1306\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/spark/python/pyspark/sql/utils.py\u001B[0m in \u001B[0;36mdeco\u001B[0;34m(*a, **kw)\u001B[0m\n\u001B[1;32m    121\u001B[0m                 \u001B[0;31m# Hide where the exception came from that shows a non-Pythonic\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    122\u001B[0m                 \u001B[0;31m# JVM exception message.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 123\u001B[0;31m                 \u001B[0;32mraise\u001B[0m \u001B[0mconverted\u001B[0m \u001B[0;32mfrom\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    124\u001B[0m             \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    125\u001B[0m                 \u001B[0;32mraise\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;31mPythonException\u001B[0m: An exception was thrown from a UDF: 'RuntimeError: SparkContext should only be created and accessed on the driver.', from <command-1699698136113742>, line 1. Full traceback below:\nTraceback (most recent call last):\n  File \"<command-1699698136113742>\", line 1, in <lambda>\n  File \"/databricks/spark/python/pyspark/sql/udf.py\", line 199, in wrapper\n    return self(*args)\n  File \"/databricks/spark/python/pyspark/sql/udf.py\", line 177, in __call__\n    judf = self._judf\n  File \"/databricks/spark/python/pyspark/sql/udf.py\", line 161, in _judf\n    self._judf_placeholder = self._create_judf()\n  File \"/databricks/spark/python/pyspark/sql/udf.py\", line 167, in _create_judf\n    spark = SparkSession.builder.getOrCreate()\n  File \"/databricks/spark/python/pyspark/sql/session.py\", line 235, in getOrCreate\n    return session\n  File \"/databricks/spark/python/pyspark/context.py\", line 401, in getOrCreate\n    return SparkContext._active_spark_context\n  File \"/databricks/spark/python/pyspark/context.py\", line 137, in __init__\n    SparkContext._assert_on_driver()\n  File \"/databricks/spark/python/pyspark/context.py\", line 1379, in _assert_on_driver\n    raise RuntimeError(\"SparkContext should only be created and accessed on the driver.\")\nRuntimeError: SparkContext should only be created and accessed on the driver.\n","errorSummary":"<span class='ansi-red-fg'>PythonException</span>: An exception was thrown from a UDF: 'RuntimeError: SparkContext should only be created and accessed on the driver.', from <command-1699698136113742>, line 1. Full traceback below:\nTraceback (most recent call last):\n  File \"<command-1699698136113742>\", line 1, in <lambda>\n  File \"/databricks/spark/python/pyspark/sql/udf.py\", line 199, in wrapper\n    return self(*args)\n  File \"/databricks/spark/python/pyspark/sql/udf.py\", line 177, in __call__\n    judf = self._judf\n  File \"/databricks/spark/python/pyspark/sql/udf.py\", line 161, in _judf\n    self._judf_placeholder = self._create_judf()\n  File \"/databricks/spark/python/pyspark/sql/udf.py\", line 167, in _create_judf\n    spark = SparkSession.builder.getOrCreate()\n  File \"/databricks/spark/python/pyspark/sql/session.py\", line 235, in getOrCreate\n    return session\n  File \"/databricks/spark/python/pyspark/context.py\", line 401, in getOrCreate\n    return SparkContext._active_spark_context\n  File \"/databricks/spark/python/pyspark/context.py\", line 137, in __init__\n    SparkContext._assert_on_driver()\n  File \"/databricks/spark/python/pyspark/context.py\", line 1379, in _assert_on_driver\n    raise RuntimeError(\"SparkContext should only be created and accessed on the driver.\")\nRuntimeError: SparkContext should only be created and accessed on the driver.\n","metadata":{},"errorTraceType":"ansi","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/plain":["\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mPythonException\u001B[0m                           Traceback (most recent call last)\n\u001B[0;32m<command-1699698136113742>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[0mIncrement\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mudf\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;32mlambda\u001B[0m \u001B[0mx\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0my\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0mz\u001B[0m \u001B[0;34m:\u001B[0m \u001B[0mIncrement\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mx\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0my\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0mz\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mDoubleType\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 2\u001B[0;31m \u001B[0mdf\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mwithColumn\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"Increment\"\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mIncrement\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdf\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mstate\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0mdf\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msalary\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0mdf\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mbonus\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mshow\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\n\u001B[0;32m/databricks/spark/python/pyspark/sql/dataframe.py\u001B[0m in \u001B[0;36mshow\u001B[0;34m(self, n, truncate, vertical)\u001B[0m\n\u001B[1;32m    500\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    501\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0misinstance\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtruncate\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mbool\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32mand\u001B[0m \u001B[0mtruncate\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 502\u001B[0;31m             \u001B[0mprint\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_jdf\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mshowString\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mn\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;36m20\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mvertical\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    503\u001B[0m         \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    504\u001B[0m             \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.1-src.zip/py4j/java_gateway.py\u001B[0m in \u001B[0;36m__call__\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m   1302\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1303\u001B[0m         \u001B[0manswer\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mgateway_client\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msend_command\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcommand\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1304\u001B[0;31m         return_value = get_return_value(\n\u001B[0m\u001B[1;32m   1305\u001B[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001B[1;32m   1306\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/spark/python/pyspark/sql/utils.py\u001B[0m in \u001B[0;36mdeco\u001B[0;34m(*a, **kw)\u001B[0m\n\u001B[1;32m    121\u001B[0m                 \u001B[0;31m# Hide where the exception came from that shows a non-Pythonic\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    122\u001B[0m                 \u001B[0;31m# JVM exception message.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 123\u001B[0;31m                 \u001B[0;32mraise\u001B[0m \u001B[0mconverted\u001B[0m \u001B[0;32mfrom\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    124\u001B[0m             \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    125\u001B[0m                 \u001B[0;32mraise\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;31mPythonException\u001B[0m: An exception was thrown from a UDF: 'RuntimeError: SparkContext should only be created and accessed on the driver.', from <command-1699698136113742>, line 1. Full traceback below:\nTraceback (most recent call last):\n  File \"<command-1699698136113742>\", line 1, in <lambda>\n  File \"/databricks/spark/python/pyspark/sql/udf.py\", line 199, in wrapper\n    return self(*args)\n  File \"/databricks/spark/python/pyspark/sql/udf.py\", line 177, in __call__\n    judf = self._judf\n  File \"/databricks/spark/python/pyspark/sql/udf.py\", line 161, in _judf\n    self._judf_placeholder = self._create_judf()\n  File \"/databricks/spark/python/pyspark/sql/udf.py\", line 167, in _create_judf\n    spark = SparkSession.builder.getOrCreate()\n  File \"/databricks/spark/python/pyspark/sql/session.py\", line 235, in getOrCreate\n    return session\n  File \"/databricks/spark/python/pyspark/context.py\", line 401, in getOrCreate\n    return SparkContext._active_spark_context\n  File \"/databricks/spark/python/pyspark/context.py\", line 137, in __init__\n    SparkContext._assert_on_driver()\n  File \"/databricks/spark/python/pyspark/context.py\", line 1379, in _assert_on_driver\n    raise RuntimeError(\"SparkContext should only be created and accessed on the driver.\")\nRuntimeError: SparkContext should only be created and accessed on the driver.\n"]}}],"execution_count":0},{"cell_type":"code","source":["# We need to make sure that the UDF variable should not be same with the function name, else the above error is thrown.\n# The correct approach is as below :\n\nIncrement_Udf = udf(lambda x,y,z : Increment(x,y,z), DoubleType())\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"0b28b4ef-2134-4521-b48c-04b363d2d4f1"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["df.withColumn(\"Increment\", Increment_Udf(df.state,df.salary,df.bonus)).show()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f9953239-b213-486a-b98c-6fdb59c20584"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"Traceback (most recent call last):\n  File \"/databricks/spark/python/pyspark/serializers.py\", line 473, in dumps\n    return cloudpickle.dumps(obj, pickle_protocol)\n  File \"/databricks/spark/python/pyspark/cloudpickle/cloudpickle_fast.py\", line 73, in dumps\n    cp.dump(obj)\n  File \"/databricks/spark/python/pyspark/cloudpickle/cloudpickle_fast.py\", line 563, in dump\n    return Pickler.dump(self, obj)\nTypeError: cannot pickle '_thread.RLock' object\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["Traceback (most recent call last):\n  File \"/databricks/spark/python/pyspark/serializers.py\", line 473, in dumps\n    return cloudpickle.dumps(obj, pickle_protocol)\n  File \"/databricks/spark/python/pyspark/cloudpickle/cloudpickle_fast.py\", line 73, in dumps\n    cp.dump(obj)\n  File \"/databricks/spark/python/pyspark/cloudpickle/cloudpickle_fast.py\", line 563, in dump\n    return Pickler.dump(self, obj)\nTypeError: cannot pickle '_thread.RLock' object\n"]}},{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)\n\u001B[0;32m/databricks/spark/python/pyspark/serializers.py\u001B[0m in \u001B[0;36mdumps\u001B[0;34m(self, obj)\u001B[0m\n\u001B[1;32m    472\u001B[0m         \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 473\u001B[0;31m             \u001B[0;32mreturn\u001B[0m \u001B[0mcloudpickle\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdumps\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mobj\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mpickle_protocol\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    474\u001B[0m         \u001B[0;32mexcept\u001B[0m \u001B[0mpickle\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mPickleError\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/spark/python/pyspark/cloudpickle/cloudpickle_fast.py\u001B[0m in \u001B[0;36mdumps\u001B[0;34m(obj, protocol, buffer_callback)\u001B[0m\n\u001B[1;32m     72\u001B[0m             )\n\u001B[0;32m---> 73\u001B[0;31m             \u001B[0mcp\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdump\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mobj\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     74\u001B[0m             \u001B[0;32mreturn\u001B[0m \u001B[0mfile\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mgetvalue\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/spark/python/pyspark/cloudpickle/cloudpickle_fast.py\u001B[0m in \u001B[0;36mdump\u001B[0;34m(self, obj)\u001B[0m\n\u001B[1;32m    562\u001B[0m         \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 563\u001B[0;31m             \u001B[0;32mreturn\u001B[0m \u001B[0mPickler\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdump\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mobj\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    564\u001B[0m         \u001B[0;32mexcept\u001B[0m \u001B[0mRuntimeError\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0me\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;31mTypeError\u001B[0m: cannot pickle '_thread.RLock' object\n\nDuring handling of the above exception, another exception occurred:\n\n\u001B[0;31mPicklingError\u001B[0m                             Traceback (most recent call last)\n\u001B[0;32m<command-1699698136113745>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[0;32m----> 1\u001B[0;31m \u001B[0mdf\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mwithColumn\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"Increment\"\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mIncrement_Udf\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdf\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mstate\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0mdf\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msalary\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0mdf\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mbonus\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mshow\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\n\u001B[0;32m/databricks/spark/python/pyspark/sql/udf.py\u001B[0m in \u001B[0;36mwrapper\u001B[0;34m(*args)\u001B[0m\n\u001B[1;32m    197\u001B[0m         \u001B[0;34m@\u001B[0m\u001B[0mfunctools\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mwraps\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfunc\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0massigned\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0massignments\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    198\u001B[0m         \u001B[0;32mdef\u001B[0m \u001B[0mwrapper\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 199\u001B[0;31m             \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    200\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    201\u001B[0m         \u001B[0mwrapper\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m__name__\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_name\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/spark/python/pyspark/sql/udf.py\u001B[0m in \u001B[0;36m__call__\u001B[0;34m(self, *cols)\u001B[0m\n\u001B[1;32m    175\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    176\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0m__call__\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m*\u001B[0m\u001B[0mcols\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 177\u001B[0;31m         \u001B[0mjudf\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_judf\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    178\u001B[0m         \u001B[0msc\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mSparkContext\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_active_spark_context\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    179\u001B[0m         \u001B[0;32mreturn\u001B[0m \u001B[0mColumn\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mjudf\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mapply\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0m_to_seq\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0msc\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mcols\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0m_to_java_column\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/spark/python/pyspark/sql/udf.py\u001B[0m in \u001B[0;36m_judf\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    159\u001B[0m         \u001B[0;31m# and should have a minimal performance impact.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    160\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_judf_placeholder\u001B[0m \u001B[0;32mis\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 161\u001B[0;31m             \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_judf_placeholder\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_create_judf\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    162\u001B[0m         \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_judf_placeholder\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    163\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/spark/python/pyspark/sql/udf.py\u001B[0m in \u001B[0;36m_create_judf\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    168\u001B[0m         \u001B[0msc\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mspark\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msparkContext\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    169\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 170\u001B[0;31m         \u001B[0mwrapped_func\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0m_wrap_function\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0msc\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfunc\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mreturnType\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    171\u001B[0m         \u001B[0mjdt\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mspark\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_jsparkSession\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mparseDataType\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mreturnType\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mjson\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    172\u001B[0m         judf = sc._jvm.org.apache.spark.sql.execution.python.UserDefinedPythonFunction(\n\n\u001B[0;32m/databricks/spark/python/pyspark/sql/udf.py\u001B[0m in \u001B[0;36m_wrap_function\u001B[0;34m(sc, func, returnType)\u001B[0m\n\u001B[1;32m     32\u001B[0m \u001B[0;32mdef\u001B[0m \u001B[0m_wrap_function\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0msc\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mfunc\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mreturnType\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     33\u001B[0m     \u001B[0mcommand\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m(\u001B[0m\u001B[0mfunc\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mreturnType\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 34\u001B[0;31m     \u001B[0mpickled_command\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mbroadcast_vars\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0menv\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mincludes\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0m_prepare_for_python_RDD\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0msc\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mcommand\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     35\u001B[0m     return sc._jvm.PythonFunction(bytearray(pickled_command), env, includes, sc.pythonExec,\n\u001B[1;32m     36\u001B[0m                                   sc.pythonVer, broadcast_vars, sc._javaAccumulator)\n\n\u001B[0;32m/databricks/spark/python/pyspark/rdd.py\u001B[0m in \u001B[0;36m_prepare_for_python_RDD\u001B[0;34m(sc, command)\u001B[0m\n\u001B[1;32m   2848\u001B[0m     \u001B[0;31m# the serialized command will be compressed by broadcast\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   2849\u001B[0m     \u001B[0mser\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mCloudPickleSerializer\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 2850\u001B[0;31m     \u001B[0mpickled_command\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mser\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdumps\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcommand\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   2851\u001B[0m     \u001B[0;32mif\u001B[0m \u001B[0mlen\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mpickled_command\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m>\u001B[0m \u001B[0msc\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_jvm\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mPythonUtils\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mgetBroadcastThreshold\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0msc\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_jsc\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m  \u001B[0;31m# Default 1M\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   2852\u001B[0m         \u001B[0;31m# The broadcast will have same life cycle as created PythonRDD\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/spark/python/pyspark/serializers.py\u001B[0m in \u001B[0;36mdumps\u001B[0;34m(self, obj)\u001B[0m\n\u001B[1;32m    481\u001B[0m                 \u001B[0mmsg\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m\"Could not serialize object: %s: %s\"\u001B[0m \u001B[0;34m%\u001B[0m \u001B[0;34m(\u001B[0m\u001B[0me\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m__class__\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m__name__\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0memsg\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    482\u001B[0m             \u001B[0mprint_exec\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0msys\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mstderr\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 483\u001B[0;31m             \u001B[0;32mraise\u001B[0m \u001B[0mpickle\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mPicklingError\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mmsg\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    484\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    485\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;31mPicklingError\u001B[0m: Could not serialize object: TypeError: cannot pickle '_thread.RLock' object","errorSummary":"<span class='ansi-red-fg'>PicklingError</span>: Could not serialize object: TypeError: cannot pickle '_thread.RLock' object","metadata":{},"errorTraceType":"ansi","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/plain":["\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)\n\u001B[0;32m/databricks/spark/python/pyspark/serializers.py\u001B[0m in \u001B[0;36mdumps\u001B[0;34m(self, obj)\u001B[0m\n\u001B[1;32m    472\u001B[0m         \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 473\u001B[0;31m             \u001B[0;32mreturn\u001B[0m \u001B[0mcloudpickle\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdumps\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mobj\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mpickle_protocol\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    474\u001B[0m         \u001B[0;32mexcept\u001B[0m \u001B[0mpickle\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mPickleError\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/spark/python/pyspark/cloudpickle/cloudpickle_fast.py\u001B[0m in \u001B[0;36mdumps\u001B[0;34m(obj, protocol, buffer_callback)\u001B[0m\n\u001B[1;32m     72\u001B[0m             )\n\u001B[0;32m---> 73\u001B[0;31m             \u001B[0mcp\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdump\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mobj\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     74\u001B[0m             \u001B[0;32mreturn\u001B[0m \u001B[0mfile\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mgetvalue\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/spark/python/pyspark/cloudpickle/cloudpickle_fast.py\u001B[0m in \u001B[0;36mdump\u001B[0;34m(self, obj)\u001B[0m\n\u001B[1;32m    562\u001B[0m         \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 563\u001B[0;31m             \u001B[0;32mreturn\u001B[0m \u001B[0mPickler\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdump\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mobj\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    564\u001B[0m         \u001B[0;32mexcept\u001B[0m \u001B[0mRuntimeError\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0me\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;31mTypeError\u001B[0m: cannot pickle '_thread.RLock' object\n\nDuring handling of the above exception, another exception occurred:\n\n\u001B[0;31mPicklingError\u001B[0m                             Traceback (most recent call last)\n\u001B[0;32m<command-1699698136113745>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[0;32m----> 1\u001B[0;31m \u001B[0mdf\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mwithColumn\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"Increment\"\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mIncrement_Udf\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdf\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mstate\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0mdf\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msalary\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0mdf\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mbonus\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mshow\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\n\u001B[0;32m/databricks/spark/python/pyspark/sql/udf.py\u001B[0m in \u001B[0;36mwrapper\u001B[0;34m(*args)\u001B[0m\n\u001B[1;32m    197\u001B[0m         \u001B[0;34m@\u001B[0m\u001B[0mfunctools\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mwraps\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfunc\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0massigned\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0massignments\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    198\u001B[0m         \u001B[0;32mdef\u001B[0m \u001B[0mwrapper\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 199\u001B[0;31m             \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    200\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    201\u001B[0m         \u001B[0mwrapper\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m__name__\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_name\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/spark/python/pyspark/sql/udf.py\u001B[0m in \u001B[0;36m__call__\u001B[0;34m(self, *cols)\u001B[0m\n\u001B[1;32m    175\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    176\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0m__call__\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m*\u001B[0m\u001B[0mcols\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 177\u001B[0;31m         \u001B[0mjudf\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_judf\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    178\u001B[0m         \u001B[0msc\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mSparkContext\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_active_spark_context\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    179\u001B[0m         \u001B[0;32mreturn\u001B[0m \u001B[0mColumn\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mjudf\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mapply\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0m_to_seq\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0msc\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mcols\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0m_to_java_column\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/spark/python/pyspark/sql/udf.py\u001B[0m in \u001B[0;36m_judf\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    159\u001B[0m         \u001B[0;31m# and should have a minimal performance impact.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    160\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_judf_placeholder\u001B[0m \u001B[0;32mis\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 161\u001B[0;31m             \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_judf_placeholder\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_create_judf\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    162\u001B[0m         \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_judf_placeholder\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    163\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/spark/python/pyspark/sql/udf.py\u001B[0m in \u001B[0;36m_create_judf\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    168\u001B[0m         \u001B[0msc\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mspark\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msparkContext\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    169\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 170\u001B[0;31m         \u001B[0mwrapped_func\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0m_wrap_function\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0msc\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfunc\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mreturnType\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    171\u001B[0m         \u001B[0mjdt\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mspark\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_jsparkSession\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mparseDataType\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mreturnType\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mjson\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    172\u001B[0m         judf = sc._jvm.org.apache.spark.sql.execution.python.UserDefinedPythonFunction(\n\n\u001B[0;32m/databricks/spark/python/pyspark/sql/udf.py\u001B[0m in \u001B[0;36m_wrap_function\u001B[0;34m(sc, func, returnType)\u001B[0m\n\u001B[1;32m     32\u001B[0m \u001B[0;32mdef\u001B[0m \u001B[0m_wrap_function\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0msc\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mfunc\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mreturnType\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     33\u001B[0m     \u001B[0mcommand\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m(\u001B[0m\u001B[0mfunc\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mreturnType\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 34\u001B[0;31m     \u001B[0mpickled_command\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mbroadcast_vars\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0menv\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mincludes\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0m_prepare_for_python_RDD\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0msc\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mcommand\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     35\u001B[0m     return sc._jvm.PythonFunction(bytearray(pickled_command), env, includes, sc.pythonExec,\n\u001B[1;32m     36\u001B[0m                                   sc.pythonVer, broadcast_vars, sc._javaAccumulator)\n\n\u001B[0;32m/databricks/spark/python/pyspark/rdd.py\u001B[0m in \u001B[0;36m_prepare_for_python_RDD\u001B[0;34m(sc, command)\u001B[0m\n\u001B[1;32m   2848\u001B[0m     \u001B[0;31m# the serialized command will be compressed by broadcast\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   2849\u001B[0m     \u001B[0mser\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mCloudPickleSerializer\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 2850\u001B[0;31m     \u001B[0mpickled_command\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mser\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdumps\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcommand\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   2851\u001B[0m     \u001B[0;32mif\u001B[0m \u001B[0mlen\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mpickled_command\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m>\u001B[0m \u001B[0msc\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_jvm\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mPythonUtils\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mgetBroadcastThreshold\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0msc\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_jsc\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m  \u001B[0;31m# Default 1M\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   2852\u001B[0m         \u001B[0;31m# The broadcast will have same life cycle as created PythonRDD\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/spark/python/pyspark/serializers.py\u001B[0m in \u001B[0;36mdumps\u001B[0;34m(self, obj)\u001B[0m\n\u001B[1;32m    481\u001B[0m                 \u001B[0mmsg\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m\"Could not serialize object: %s: %s\"\u001B[0m \u001B[0;34m%\u001B[0m \u001B[0;34m(\u001B[0m\u001B[0me\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m__class__\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m__name__\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0memsg\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    482\u001B[0m             \u001B[0mprint_exec\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0msys\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mstderr\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 483\u001B[0;31m             \u001B[0;32mraise\u001B[0m \u001B[0mpickle\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mPicklingError\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mmsg\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    484\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    485\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;31mPicklingError\u001B[0m: Could not serialize object: TypeError: cannot pickle '_thread.RLock' object"]}}],"execution_count":0},{"cell_type":"code","source":["# The above error will come, if the function definition is not executed\n# We need to create and execute function before, we create the udf object"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c5c1e8ff-8936-4ceb-909c-c79562e51553"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["df.withColumn(\"Increment\", Increment_Udf(df.state,df.salary,df.bonus)).show()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"dd268a83-7af8-48bc-bf04-1f019b84cf35"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"+-------------+----------+-----+------+---+-----+---------+\n|employee_name|department|state|salary|age|bonus|Increment|\n+-------------+----------+-----+------+---+-----+---------+\n|        James|     Sales|   NY| 90000| 34|10000|   9500.0|\n|      Michael|     Sales|   NY| 86000| 56|20000|   9600.0|\n|       Robert|     Sales|   CA| 81000| 30|23000|  10410.0|\n|        Maria|   Finance|   CA| 90000| 24|23000|  11490.0|\n|        Raman|   Finance|   CA| 99000| 40|24000|  12600.0|\n|        Scott|   Finance|   NY| 83000| 36|19000|   9250.0|\n|          Jen|   Finance|   NY| 79000| 53|15000|   8650.0|\n|         Jeff| Marketing|   CA| 80000| 25|18000|  10140.0|\n|        Kumar| Marketing|   NY| 91000| 50|21000|  10150.0|\n+-------------+----------+-----+------+---+-----+---------+\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+-------------+----------+-----+------+---+-----+---------+\n|employee_name|department|state|salary|age|bonus|Increment|\n+-------------+----------+-----+------+---+-----+---------+\n|        James|     Sales|   NY| 90000| 34|10000|   9500.0|\n|      Michael|     Sales|   NY| 86000| 56|20000|   9600.0|\n|       Robert|     Sales|   CA| 81000| 30|23000|  10410.0|\n|        Maria|   Finance|   CA| 90000| 24|23000|  11490.0|\n|        Raman|   Finance|   CA| 99000| 40|24000|  12600.0|\n|        Scott|   Finance|   NY| 83000| 36|19000|   9250.0|\n|          Jen|   Finance|   NY| 79000| 53|15000|   8650.0|\n|         Jeff| Marketing|   CA| 80000| 25|18000|  10140.0|\n|        Kumar| Marketing|   NY| 91000| 50|21000|  10150.0|\n+-------------+----------+-----+------+---+-----+---------+\n\n"]}}],"execution_count":0},{"cell_type":"code","source":[""],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"7c8e642d-c789-4b84-9342-76f492b8b7da"}},"outputs":[],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"pySpark- Assignment - UDF","dashboards":[],"notebookMetadata":{"pythonIndentUnit":2},"language":"python","widgets":{},"notebookOrigID":1585798411456918}},"nbformat":4,"nbformat_minor":0}
